# vision-LLMs

Exploring the use of V-LLMs.

## Description

Implement real-time video inference with V-LLMs based on the nanoowl NVIDIA optimized models.

## Requirements

 - Docker and Docker Compose
 - NVIDIA GPU and appropriate drivers and containers toolkits
 - Compatible nanoowl image encoder engine  
 
## Usage

```bash
    docker compose up --build 
```
